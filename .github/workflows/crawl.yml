name: Crawl TopCV Jobs

on:
  # Run on schedule (UTC time)
  schedule:
    # Run daily at 6:00 AM UTC (1:00 PM Vietnam time)
    - cron: '0 6 * * *'
  
  # Allow manual trigger
  workflow_dispatch:
    inputs:
      keywords:
        description: 'Keywords to search (comma-separated)'
        required: false
        default: 'Data Analyst,Data Engineer,Data Scientist,Backend Developer,Frontend Developer,DevOps Engineer'
      start_page:
        description: 'Start page'
        required: false
        default: '1'
      end_page:
        description: 'End page'
        required: false
        default: '3'

env:
  PYTHON_VERSION: '3.11'

jobs:
  crawl:
    runs-on: ubuntu-latest
    
    steps:
      - name: ðŸ“¥ Checkout repository
        uses: actions/checkout@v4
      
      - name: ðŸ Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'
      
      - name: ðŸ“¦ Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
      
      - name: ðŸ•·ï¸ Run crawler
        env:
          GDRIVE_CREDENTIALS: ${{ secrets.GDRIVE_CREDENTIALS }}
          GDRIVE_FOLDER_ID: ${{ secrets.GDRIVE_FOLDER_ID }}
        run: |
          # Set keywords from input or use defaults
          if [ "${{ github.event_name }}" == "workflow_dispatch" ]; then
            KEYWORDS="${{ github.event.inputs.keywords }}"
            START_PAGE="${{ github.event.inputs.start_page }}"
            END_PAGE="${{ github.event.inputs.end_page }}"
          else
            KEYWORDS="Data Analyst,Data Engineer,Data Scientist,Backend Developer,Frontend Developer,DevOps Engineer,QA Engineer,Software Engineer,Machine Learning,Python Developer"
            START_PAGE="1"
            END_PAGE="3"
          fi
          
          # Convert comma-separated to space-separated for argparse
          KEYWORDS_ARGS=$(echo "$KEYWORDS" | tr ',' '\n' | sed 's/^/"/' | sed 's/$/"/' | tr '\n' ' ')
          
          echo "ðŸ” Keywords: $KEYWORDS"
          echo "ðŸ“„ Pages: $START_PAGE - $END_PAGE"
          
          # Run the crawler
          python -m src.main \
            --keywords $KEYWORDS_ARGS \
            --start-page $START_PAGE \
            --end-page $END_PAGE \
            --output-dir ./data \
            --upload-gdrive
      
      - name: ðŸ“Š Upload artifacts
        uses: actions/upload-artifact@v4
        with:
          name: crawl-data-${{ github.run_id }}
          path: data/
          retention-days: 30
      
      - name: ðŸ“ Summary
        run: |
          echo "## ðŸ•·ï¸ Crawl Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "ðŸ“… Date: $(date +%Y-%m-%d)" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### Files created:" >> $GITHUB_STEP_SUMMARY
          ls -la data/*.csv 2>/dev/null | awk '{print "- " $NF}' >> $GITHUB_STEP_SUMMARY || echo "No CSV files created" >> $GITHUB_STEP_SUMMARY
