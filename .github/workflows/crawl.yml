name: TopCV Job Crawler

on:
  # Run on schedule (UTC time)
  schedule:
    # Run daily at 1:00 AM UTC (8:00 AM Vietnam time)
    - cron: '0 18 * * *'
  
  # Allow manual trigger
  workflow_dispatch:
    inputs:
      keywords:
        description: 'Keywords (semicolon-separated, e.g., "Data Analyst;Python Developer")'
        required: false
        default: 'Data Analyst;Data Engineer;Python Developer'
      start_page:
        description: 'Start page'
        required: false
        default: '1'
      end_page:
        description: 'End page'
        required: false
        default: '2'

env:
  PYTHON_VERSION: '3.11'

jobs:
  crawl:
    runs-on: ubuntu-latest
    
    steps:
      - name: ðŸ“¥ Checkout repository
        uses: actions/checkout@v4
      
      - name: ðŸ Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'
      
      - name: ðŸ“¦ Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: ðŸ“… Set crawl date (Vietnam timezone)
        id: date
        run: |
          CRAWL_DATE=$(TZ='Asia/Ho_Chi_Minh' date '+%Y-%m-%d')
          echo "crawl_date=$CRAWL_DATE" >> $GITHUB_OUTPUT
          echo "ðŸ“… Crawl date (Vietnam): $CRAWL_DATE"
      
      - name: ðŸ•·ï¸ Run crawler
        env:
          GDRIVE_CREDENTIALS: ${{ secrets.GDRIVE_CREDENTIALS }}
          GDRIVE_FOLDER_ID: ${{ secrets.GDRIVE_FOLDER_ID }}
        run: |
          # Set keywords from input or use defaults (semicolon-separated)
          if [ "${{ github.event_name }}" == "workflow_dispatch" ]; then
            KEYWORDS="${{ github.event.inputs.keywords }}"
            START_PAGE="${{ github.event.inputs.start_page }}"
            END_PAGE="${{ github.event.inputs.end_page }}"
          else
            KEYWORDS="Data Analyst;Data Engineer;Python Developer;Backend Developer;Frontend Developer"
            START_PAGE="1"
            END_PAGE="2"
          fi
          
          echo "============================================"
          echo "ðŸ” Keywords: $KEYWORDS"
          echo "ðŸ“„ Pages: $START_PAGE - $END_PAGE"
          echo "ðŸ“… Crawl Date: ${{ steps.date.outputs.crawl_date }}"
          echo "============================================"
          
          # Run the crawler
          python -m src.main \
            --keywords "$KEYWORDS" \
            --start-page "$START_PAGE" \
            --end-page "$END_PAGE" \
            --output data \
            --crawl-date "${{ steps.date.outputs.crawl_date }}"

      - name: â˜ï¸ Upload to Google Drive
        if: success()
        env:
          GDRIVE_CREDENTIALS: ${{ secrets.GDRIVE_CREDENTIALS }}
          GDRIVE_FOLDER_ID: ${{ secrets.GDRIVE_FOLDER_ID }}
        run: |
          if [ -n "$GDRIVE_CREDENTIALS" ] && [ -n "$GDRIVE_FOLDER_ID" ]; then
            if ls data/*.csv 1> /dev/null 2>&1 || ls data/*.xlsx 1> /dev/null 2>&1; then
              echo "ðŸ“¤ Uploading files to Google Drive..."
              python -m src.upload_gdrive
            else
              echo "âš ï¸ No data files to upload"
            fi
          else
            echo "âš ï¸ Google Drive credentials not configured, skipping upload"
          fi
      
      - name: ðŸ“Š Upload artifacts
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: crawl-data-${{ steps.date.outputs.crawl_date }}
          path: data/
          retention-days: 30
      
      - name: ðŸ“ Summary
        run: |
          echo "## ðŸ•·ï¸ Crawl Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "ðŸ“… Date: ${{ steps.date.outputs.crawl_date }}" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### Files created:" >> $GITHUB_STEP_SUMMARY
          ls -la data/*.csv 2>/dev/null | awk '{print "- " $NF}' >> $GITHUB_STEP_SUMMARY || echo "No CSV files created" >> $GITHUB_STEP_SUMMARY
